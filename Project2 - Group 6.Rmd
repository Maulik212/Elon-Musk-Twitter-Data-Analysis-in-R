---
title: "FDA Project2 Group 6"
output: html_document
---

```{r}
library(dplyr)
library(stringr)
library(tidytext)
library(janeaustenr)
library(ggplot2)
library(tidyr)
library(igraph)
library(ggraph)
library(tidyverse)
library(lubridate)
library(stopwords)
library(tibble)

```


```{r}
# Task 1

######################## CODE TAKEN FROM Assignment1 Task 3.1 ############

library(stringr)
library(dplyr)
keyword_data <- read.csv("Keyword_data - Keyword_data.csv")

index_vec <- c()
index_vec_inc <- 1
row_cnt <- 0
for (row_val in keyword_data$Title){
 
  row_cnt <- row_cnt + 1
  if ( row_val == '' )
    {
     
      index_vec[index_vec_inc] <- row_cnt
      index_vec_inc <- index_vec_inc + 1
       
    }
  else if( any (str_detect(row_val, (paste(month.abb, "/", sep = '')) , negate = FALSE ) ) == TRUE )
    {
   
      index_vec[index_vec_inc] <- row_cnt
      index_vec_inc <- index_vec_inc + 1
    }
 
}

keyword_data_clean <- keyword_data[-c(index_vec),]


## Remove the Title column from the data frame
keyword_data_clean <- keyword_data_clean[, 2:13]

##keyword_data_clean <- keyword_data_clean[]

len_keyword <- length(keyword_data_clean)
no_of_articles <- dim(keyword_data_clean)[1]


## Count Number of Unique Keywords

cnt_for_keyword = 1
list_uniq_keyword <- list()
for (article in 1:no_of_articles){

  for(keyword in 1:len_keyword){
    list_uniq_keyword[[cnt_for_keyword]] <- keyword_data_clean[[keyword]][[article]]

    #keyword_data_clean[[j]][[i]]
    cnt_for_keyword <- cnt_for_keyword + 1
  }
}


  #Removed Duplicate values
list_uniq_keyword <- unique((list_uniq_keyword))

list_uniq_keyword <- list_uniq_keyword[!list_uniq_keyword == '']

#  list_uniq_keyword[1:10]
length_uniq_keywords <- length(list_uniq_keyword)


# Creating a 248*248 matrix

keyword_matrix <- matrix(0, nrow = length_uniq_keywords, ncol = length_uniq_keywords)

# Name the rows & cols of the matrix
colnames(keyword_matrix) <- list_uniq_keyword
rownames(keyword_matrix) <- list_uniq_keyword



# Iteration through each unique keyword

# For each article
for (article in 1:no_of_articles){
 
  # For each keyword in an article  
  for (keyword_cnt in 1:(len_keyword-1) ){
    current_keyword <- keyword_data_clean[[keyword_cnt]][[article]]
   
    if (current_keyword == ''){
       next
    }
   
    # For next keyword in an article
    for (next_keyword_cnt in (keyword_cnt+1):len_keyword){
      next_keyword <- keyword_data_clean[[next_keyword_cnt]][[article]]
     
      if (next_keyword == ''){
       
        next
      }
   
      keyword_matrix[current_keyword, next_keyword] <- keyword_matrix[current_keyword, next_keyword] + 1 -> keyword_matrix[next_keyword, current_keyword]  
    }
   
   
  }
 
 
}

#########################

# 1.2

sum(keyword_matrix)


library(igraph)

net1 <- graph_from_adjacency_matrix(as.matrix(keyword_matrix),mode="undirected",weighted = T)
plot(net1)

# 1.3

#  Degree of network
deg <- degree(net1, mode = "all")
d1 <- as.data.frame(deg)
d1

# Strength of network
strength <- strength(net1, mode = "all")
d2 <- as.data.frame(strength)
d2

# 1.4

# Top 10 nodes by degree
d1 %>% top_n(10)
Deg_10 <- d1 %>% slice_max(deg, n = 10)
Deg_10

# Top 10 nodes by strength
d2 %>% top_n(10)
Str_10 <- d2 %>% slice_max(strength, n = 10)
Str_10

# 1.5

W <-  get.data.frame(net1)

# Top 10 node pairs by weight

W1 <- head(W[order(W$weight, decreasing=TRUE), ], 10)
rownames(W1) <- c("1","2","3","4","5","6","7","8","9","10")
W1

# 1.6

library(ggplot2)

D0 <- as.data.frame(deg)
D1 <- as.data.frame(strength)

D0 <- cbind(D0,D1$strength)
colnames(D0)[2] <- "strength"

New_df <- D0 %>%
  group_by(deg) %>%
  summarise(Average_strength = mean(strength))

# Plotting Degree vs Average strength

plot(x= New_df$deg, y = New_df$Average_strength , main="Degree vs Average Strength",
     xlab="Degree", ylab="Average_Strength")


```

```{r}

# Task 2

```


```{r}

data <- read.csv("2021.csv")

el_tweets <- data %>%
               select(date, tweet)

el_tweets$date <- as.Date(el_tweets$date)

el_tweets$year <- as.numeric(format(el_tweets$date, "%Y"))


el_tweets <- el_tweets %>%
               filter(year >= 2017)

```


```{r}

# Few customized stop words
customized_stop_words <- data.frame('word' = c("http","https","t.co", "amp" , "itâ" , "â"))


tw <- el_tweets %>%
  unnest_tokens(word, tweet) %>%
  anti_join(stop_words) %>%
  filter(!word %in% customized_stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, year, sort = TRUE)


# Print Top 10 highest word count
# For 2017 data
tw_2017 <- tw[tw$year == '2017',]
head(tw_2017,10)

# For 2018 data
tw_2018 <- tw[tw$year == '2018',]
head(tw_2018,10)

# For 2019 data
tw_2019 <- tw[tw$year == '2019',]
head(tw_2019,10)

# For 2020 data
tw_2020 <- tw[tw$year == '2020',]
head(tw_2020,10)

# For 2021 data
tw_2021 <- tw[tw$year == '2021',]
head(tw_2021,10)


# Calculate Total Count for each year
total_tw <- tw %>% 
  group_by(year) %>% 
  summarize(total = sum(n))

tweet_words <- left_join(tw, total_tw)


ggplot(tweet_words, aes(n/total, fill = year)) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~year, ncol = 2, scales = "free_y")



```

```{r}

# Zipf's law
freq_by_rank <- tweet_words %>% 
  group_by(year) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = year)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = TRUE) + 
  scale_x_log10() +
  scale_y_log10()

rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = year)) + 
  geom_abline(intercept = -0.62, slope = -1.1, 
              color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = TRUE) + 
  scale_x_log10() +
  scale_y_log10()

```
```{r}


# Bi-gram Network
el_tweets_bigram <- data %>%
    select(date,tweet) %>%
    unnest_tokens(bigram, tweet, token = "ngrams", n = 2)

el_tweets_bigram$date <- as.Date(el_tweets_bigram$date)

el_tweets_bigram$year <- as.numeric(format(el_tweets_bigram$date, "%Y"))


el_tweets_bigram <- el_tweets_bigram %>%
  select(bigram,year) %>%
  filter(year >= 2017)


el_tweets_bigram %>%
  count(bigram, sort = TRUE)

# Calculating bigrams with stop words
el_bigrams_separated <- el_tweets_bigram %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# Removing Stop words from bigram
sw <- data.frame('word' = stop_words$word)
cust_stop_words <- rbind(customized_stop_words,sw)

el_bigrams_filtered <- el_bigrams_separated %>%
  filter(!word1 %in% cust_stop_words$word) %>%
  filter(!word2 %in% cust_stop_words$word) %>%
  filter(str_detect(word1, "[a-z]")) %>%
  filter(str_detect(word2, "[a-z]"))


# new bigram counts:
el_bigram_counts <- el_bigrams_filtered %>% 
  group_by(year) %>%
  count(word1, word2, sort = TRUE) 

# bigram as tf-idf
el_bigrams_united <- el_bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

el_bigram_tf_idf <- el_bigrams_united %>%
  count(year, bigram) %>%
  bind_tf_idf(bigram, year, n) %>%
  arrange(desc(tf_idf))

#---------

# Visualizing bigrams
library(igraph)
el_bigram_counts

bi_e_2017 <- el_bigram_counts %>%
  filter(year == 2017)

bi_e_2018 <- el_bigram_counts %>%
  filter(year == 2018)

bi_e_2019 <- el_bigram_counts %>%
  filter(year == 2019)

bi_e_2020 <- el_bigram_counts %>%
  filter(year == 2020)

bi_e_2021 <- el_bigram_counts %>%
  filter(year == 2021)

bigram_graph_2017 <- bi_e_2017 %>%
  filter(n > 5) %>%
  graph_from_data_frame()

bigram_graph_2018 <- bi_e_2018 %>%
  filter(n > 10) %>%
  graph_from_data_frame()

bigram_graph_2019 <- bi_e_2019 %>%
  filter(n > 11) %>%
  graph_from_data_frame()

bigram_graph_2020 <- bi_e_2020 %>%
  filter(n > 8) %>%
  graph_from_data_frame()

bigram_graph_2021 <- bi_e_2021 %>%
  filter(n > 4) %>%
  graph_from_data_frame()


a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

# Plot Biagrams

ggraph(bigram_graph_2017, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()


ggraph(bigram_graph_2018, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()


ggraph(bigram_graph_2019, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()


ggraph(bigram_graph_2020, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()


ggraph(bigram_graph_2021, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()



```




